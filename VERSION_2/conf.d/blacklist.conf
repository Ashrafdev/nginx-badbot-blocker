### *********************
### NGINX BAD BOT BLOCKER
### *********************

### Version 2.2017.02

### Project URL: https://github.com/mariusv/nginx-badbot-blocker

### This file implements a checklist / blacklist for good user agents, bad user agents and
### bad referrers. It also has whitelisting for your own IP's and known good IP Ranges
### and also has rate limiting functionality for bad bots who you only want to rate limit
### and not actually block out entirely. It is powerful and also flexible.

### Version 1 Created By: https://github.com/mariusv/
### Version 2 Created By: https://github.com/mitchellkrogza/
### Copyright MariusV <myself@mariusv.com> and Mitchell Krog <mitchellkrog@gmail.com>

### Version 2 has been rewritten from the ground up by mitchellkrogza who first started off using
### the original Nginx Bot Blocker created by MariusV. Due to formatting issues, complicated and uneccessary
### Regex patterns and inabilities to track commits properly, the entire layout was re-designed
### bringing to all existing users a new super powerful version of the bot blocker which is much easier to maintain and also to update. 

### LAST UPDATED
### Updated: TUE JUL  4 17:24:44 SAST 2017
### END LAST UPDATED

### Tested on: nginx/1.10.0 (Ubuntu 16.04)

### This list was developed on and is in use on a live Nginx server running some very busy web sites.
### It was built from scratch using real data from daily logs and is updated almost daily.
### It has been extensively tested for false positives and all additions to the lists of bad user agents,
### spam referers, rogue IP address, scanners, scrapers and domain hijacking sites are extensively checked
### before they are added. It is monitored extensively for any false positives.

### *********
### Features:
### *********
###	Clear formatting for Ease of Maintenance.
###	Alphabetically ordered lists for Ease of Maintenance.
###	Extensive Commenting for Ease of Reference.
###	Extensive bad_bot list
###	Extensive bad_referer list (please excuse the nasty words and domains)
###	Simple regex patterns versus complicated messy regex patterns.
###	Checks regardless of http / https urls or the lack of any protocol sent.
###	IP range blocking / whitelisting.
###	Rate Limiting Functions.

### *** PLEASE READ ALL INLINE NOTES ON TESTING !!!!

###	We have this set up as an include in nginx.conf as
###	Include /etc/nginx/conf.d/blacklist.conf
###	This is loaded and available for any vhost to use in its config
###	Each vhost then just needs the include file mentioned below for it to take effect.

### In Most cases your nginx.conf should already have an include statement as follows
### Include /etc/nginx/conf.d/*
### If that is the case then you can ignore the above include statement as Nginx will 
### load anything in the conf.d folder and make it available to all sites.

### All you then need to do is use the include statements below in the server {} block of a vhost file for it to take effect.
# 	server {
#			#Config stuff here
#			include /etc/nginx/bots.d/blockbots.conf
#			include /etc/nginx/bots.d/ddos.conf
#			#Other config stuff here
#		 }

### Make sure to read the updated configuration Instructions regarding the whitelisting include files otherwise you will get EMERG: errors

###	Need we say, please don't just copy and paste this without reviewing what bots and
###	referers are being blocked, you may want to exclude certain of them
###	Also make SURE to whitelist your own IP's in the geo $bad_referer section.
###	Know why you are using this or why you want to use it before you do, the implications
###	are quite severe.

###	*** PLEASE READ INLINE NOTES ON TESTING !!!!

###	Note that: 
###	0 = allowed - no limits
###	1 = allowed or rate limited less restrictive
###	2 = rate limited more
###	3 = block completely

###	NEED we say always do a "sudo nginx -t" to test the config is okay after adding these
###	and if so then "sudo service nginx reload" for it to take effect.

### *** MAKE SURE TO ADD to your nginx.conf ***
### server_names_hash_bucket_size 64;
### server_names_hash_max_size 4096;
### limit_req_zone $binary_remote_addr zone=flood:50m rate=90r/s;
### limit_conn_zone $binary_remote_addr zone=addr:50m;
### to allow it to load this large set of domains into memory and to set the rate limiting zones for the DDOS filter.


### ADDING YOUR OWN BAD REFERERS
### Fork your own local copy and then
### Send a Pull Request by following the instructions in the Pull_Requests folder.


# *********************************
# FIRST BLOCK BY USER-AGENT STRINGS
# *********************************

# ***************
# PLEASE TEST !!!
# ***************

# ALWAYS test any User-Agent Strings you add here to make sure you have it right
# Use a Chrome Extension called "User-Agent Switcher for Chrome" where you can create your
# own custom lists of User-Agents and test them easily against your rules below.

# You can also use Curl to test user-agents as per example below
# curl -I http://www.yourdomain.com -A "GoogleBot"  << 200 OK
# curl -I http://www.yourdomain.com -A "80legs"  <<< 444 Dropped Connection

# Here we also allow specific User Agents to come through that we want to allow

# PLEASE NOTE: In all lists below I use Nginx case-insensitive matching ~* 
# This means regardless of how you type the word, upper or lowercase or mixed it will
# be detected by Nginx Regex. Some Names are Capitalised simply for Ease of Reading.
# Especially important for both Googlebot and googlebot to be allowed through no?

# Now we map all good and bad user agents to a variable called $bad_bot

map $http_user_agent $bad_bot {

	default		0;

# ***********************************************
# Allow Good User-Agent Strings We Know and Trust
# ***********************************************

# START GOOD BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END GOOD BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END GOOD BOTS ### DO NOT EDIT THIS LINE AT ALL ###

# **************************************************
# User-Agent Strings Allowed Throug but Rate Limited
# **************************************************
# Some people block libwww-perl, it us widely used in many valid (non rogue) agents
# I allow libwww-perl as I use it for monitoring systems with Munin but it is rate limited

# START ALLOWED BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END ALLOWED BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END ALLOWED BOTS ### DO NOT EDIT THIS LINE AT ALL ###

# **************************************************************
# Rate Limited User-Agents who get a bit aggressive on bandwidth
# **************************************************************

# START LIMITED BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END LIMITED BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END LIMITED BOTS ### DO NOT EDIT THIS LINE AT ALL ###
		
# *********************************************
# Bad User-Agent Strings That We Block Outright
# *********************************************

# START BAD BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END BAD BOTS ### DO NOT EDIT THIS LINE AT ALL ###
# END BAD BOTS ### DO NOT EDIT THIS LINE AT ALL ###
}	
	

# ****************************************
# SECOND BLOCK BY REFERER STRINGS AND URLS
# ****************************************

# Add here all referrer words and URL's that are to blocked.

# *****************
# PLEASE TEST !!!!
# *****************

# ALWAYS test referers that you add. This is done manually as follows

# curl -I http://www.yourdomain.com -e http://anything.adcash.com
# curl -I http://www.yourdomain.com -e http://www.goodwebsite.com/not-adcash
# curl -I http://www.yourdomain.com -e http://www.betterwebsite.com/not/adcash

# This uses curl to send the referer string to your site and you should see an immediate
# 403 Forbidden Error or No Response at all if you use the 444 error like I do.

# Because of case-insensitive matching any combination of capitilization in the names
# will all produce a positive hit - make sure you always test thoroughly and monitor logs
# This also does NOT check for a preceding www. nor does it check for it ending in .com
# .net .org or any long string attached at the end. It also does not care if the referer
# was sent with http https or even ftp.

# Please be careful with these and think carefully before you add new words.
# Remember we are trying to keep out the general riff-raff not kill your web sites.

# **********************************************************************
# Now we map all bad referer words below to a variable called $bad_words
# **********************************************************************

map $http_referer $bad_words {
    default 0;

# ************************
# Bad Referer Single Words
# ************************
# These are Words and Terms often found tagged onto domains or within url query strings.

}

# ************************
# Bad Referer Domain Names
# ************************

# Now a list of bad referer urls these domains or any combination of them ie .com .net
# will be blocked out. Doesn't matter if the protocol is http, https or even ftp

# This section includes:
# **********************
# Blocking of SEO company Semalt.com (now merged into this one section)
# MIRAI Botnet Domains Used for Mass Attacks
# Other known bad SEO companies and Ad Hijacking Sites  
# Sites linked to malware, adware and ransomware

# *****************
# PLEASE TEST !!!!
# *****************

# ALWAYS test referers that you add. This is done manually as follows

# curl -I http://www.yourdomain.com -e http://8gold.com

# This uses curl to send the referer string to your site and you should see an immediate
# 403 Forbidden Error or No Response at all if you use the 444 error like I do.

# Because of case-insensitive matching any combination of capitilization 
# will all produce a positive hit - make sure you always test.

# curl -I http://www.yourdomain.com -e http://NOT-8gold.com
# curl -I http://www.yourdomain.com -e http://this.is.not8gOlD.net
# curl -I http://www.yourdomain.com -e ftp://8gold.com
# curl -I http://www.yourdomain.com -e ftp://www.weare8gold.NET
# curl -I http://www.yourdomain.com -e https://subdomain.8gold.com
# curl -I http://www.yourdomain.com -e https://NOT8GolD.org

# This works exactly like the bad referer word lists above and is very strict !!!
# I have gone for the simple stricter approach which blocks all variants for those
# who just hop out and but another domain name.

# So if you see a bad referer from wearegoogle.com and you want to block them just add
# them as "~*wearegoogle.com" don't ever go and do something like "~*google.com" you will
# kill all your SEO in a week. 
# While you can fork your own copy and modify it as you please we appreciate you rather sending
# a Pull Request by following the instructions in the Pull_Requests folder.

# ***********************************************************************
# Now we map all good & bad referer urls to variable called #bad_referer
# ***********************************************************************

map $http_referer $bad_referer {
	hostnames;
	default 0;

# ************************************
# GOOD REFERERS - Spared from Checking 
# ************************************

# Add your own domain names here to spare them from referer checking (one per line)
  # Use the new include file method so any further updates will no longer require you to
  # have to keep putting your whitelisted domains here when updating. 
  # PLEASE READ UPDATED CONFIGURATION INSTRUCTIONS REGARDING THESE INCLUDE FILES

  	include /etc/nginx/bots.d/whitelist-domains.conf;

# START BAD REFERERS ### DO NOT EDIT THIS LINE AT ALL ###
# END BAD REFERERS ### DO NOT EDIT THIS LINE AT ALL ###
# END BAD REFERERS ### DO NOT EDIT THIS LINE AT ALL ###

}


# ***********************************************
# WHITELISTING AND BLACKLISTING IP ADDRESS RANGES
# ***********************************************

# Geo directive to deny and also whitelist certain ip addresses

geo $validate_client {

# ********************
# First Our Safety Net
# ********************

# Anything not matching our rules is allowed through with default 0;

	default		0;

# ***********************************
# Whitelist all your OWN IP addresses
# ***********************************

# Whitelist all your own IP addresses from any validate_client checks
# Add all your IP addresses and ranges below (one per line)
  # Use the new include file method so any further updates will no longer require you to
  # have to keep putting your whitelisted IP addresses here when updating. 
  # PLEASE READ UPDATED CONFIGURATION INSTRUCTIONS REGARDING THESE INCLUDE FILES

	include /etc/nginx/bots.d/whitelist-ips.conf;

# ***********
# Google Bots
# ***********

# For Safety Sake Google's Known BOT IP Ranges are all white listed in case you add
# anything lower down that you mistakenly picked up as a bad bot.

# START GOOGLE IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###
# END GOOGLE IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###
# END GOOGLE IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###

# *********
# Bing Bots
# *********

# START BING IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###
# END BING IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###
# END BING IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###
	
# Cyveillance / Qwest Communications
# **********************************
# I am extensively researching this subject - appears to be US government involved
# and also appears to be used by all sorts of law enforcement agencies. For one they 
# do not obey robots.txt and continually disguise their User-Agent strings. Time will
# tell if this is all correct or not.
# For now see - https://en.wikipedia.org/wiki/Cyveillance

# IMPORTANT UPDATE ON Cyveillance / Qwest Communications !!!
# **********************************************************
# I have done a lot of research on Cyveillance now and through monitoring my logs I know
# for sure what companies are using them and what they are actually looking for.
# My research has led me to understand that Cyveillance services are used by hundreds
# of companies to help them dicsover theft of copyrighted materials like images, movies
# music and other materials. I personally believe a lot of block lists who originally recommended
# blocking Cyveillance have done so to protect their torrent or p2p sites from being scanned.
# I personally have now unblocked them as image theft is a big problem of mine but if you
# do want to block Cyveillance you can simply modify the entries in the block below from "0" to "1"
# Getty Images is one such company who appears to use Cyveillance to help monitor for copyright theft.
  
# If you really do want to block them change all the 0's below to 1.

# START CYVEILLANCE BLOCK ### DO NOT EDIT THIS LINE AT ALL ###
# END CYVEILLANCE BLOCK ### DO NOT EDIT THIS LINE AT ALL ###
# END CYVEILLANCE BLOCK ### DO NOT EDIT THIS LINE AT ALL ###

# ****************
# Berkely Scanner
# ****************

# The Berkeley University has a scanner testing all over the web sending a complex
# payload an expecting a reply from servers who are infected or who just respond to such
# a payload. The payload looks similar to this
# "$\xC9\xE1\xDC\x9B+\x8F\x1C\xE71\x99\xA8\xDB6\x1E#\xBB\x19#Hx\xA7\xFD\x0F9-"
# and is sometime VERY long. You may have noticed this in your logs.
# I support research projects and all my servers respond with an error to this type of
# string so I do not block them but if you want to block just uncomment the following line 
# or email them asking them not to scan your server. They do respond.
# Visit http://169.229.3.91/ for more info

# START BERKELEY SCANNER ### DO NOT EDIT THIS LINE AT ALL ###
# END BERKELEY SCANNER ### DO NOT EDIT THIS LINE AT ALL ###
# END BERKELEY SCANNER ### DO NOT EDIT THIS LINE AT ALL ###
  
# *************************
# Wordpress Theme Detectors
# *************************
  
# START WP THEME DETECTORS ### DO NOT EDIT THIS LINE AT ALL ###
# END WP THEME DETECTORS ### DO NOT EDIT THIS LINE AT ALL ###
# END WP THEME DETECTORS ### DO NOT EDIT THIS LINE AT ALL ###

# ****************************************
# NIBBLER - SEO testing and reporting tool
# ****************************************  
# See - http://nibbler.silktide.com/

# START NIBBLER ### DO NOT EDIT THIS LINE AT ALL ###
# END NIBBLER ### DO NOT EDIT THIS LINE AT ALL ###
# END NIBBLER ### DO NOT EDIT THIS LINE AT ALL ###


# ****************************
# Known Bad IP's and IP Ranges
# ****************************

# Add any other IPs or Subnets here that you wish to block
# Although any permanent blocks should be done using Fail2Ban and IPTables and not
# hampering down Nginx with all the checks against perma-banned IP's

# START BAD IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###
# END BAD IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###
# END BAD IP RANGES ### DO NOT EDIT THIS LINE AT ALL ###  
}

# Keep own IPs out of DDOS Filter
# Add your own IP addresses and ranges below to spare them from the rate
# limiting DDOS filter (one per line)
geo $ratelimited {
	default 1;
	127.0.0.1 0;
}

# *****************************************
# MAP BAD BOTS TO OUR RATE LIMITER FUNCTION
# *****************************************

	map $bad_bot $bot_iplimit {
	0    "";
	1    "";
	2    $binary_remote_addr;
	}

# ***********************
# SET RATE LIMITING ZONES
# ***********************

# BAD BOT RATE LIMITING ZONE
# limits for Zone $bad_bot = 1
# Nothing Set - you can set a different zone limiter here if you like
# We issue a 444 response instead to all bad bots.  

# limits for Zone $bad_bot = 2
# this rate limiting will only take effect if you change any of the bots and change
# their block value from 1 to 2.
	limit_conn_zone $bot_iplimit zone=bot2_connlimit:16m;
	limit_req_zone  $bot_iplimit zone=bot2_reqlimitip:16m  rate=2r/s;

### THE END of the Long and Winding Road

### *** MAKE SURE TO ADD to your nginx.conf ***
### server_names_hash_bucket_size 64;
### server_names_hash_max_size 4096;
### limit_req_zone $binary_remote_addr zone=flood:50m rate=90r/s;
### limit_conn_zone $binary_remote_addr zone=addr:50m;
### to allow it to load this large set of domains into memory and to set the rate limiting zones for the DDOS filter.


### Using Apache? Also check out the Ultimate Apache Bad Bot Blocker on Github which is built on the very same set of lists.
### https://github.com/mitchellkrogza/apache-ultimate-bad-bot-blocker
